# -*- coding: utf-8 -*-
"""Q3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ltYFzJv7ENF_kZ2jAIBBqvXtKomFt_fH
"""

from google.colab import drive
drive.mount('/content/drive')

# Commented out IPython magic to ensure Python compatibility.
# Add the project directory to the path
# %cd /content/drive/Shared drives/CS634/CS634Project/Q3
import os, sys
sys.path.append(os.getcwd())

#Importing necessary library 
import sklearn
import sklearn.ensemble
import sklearn.metrics
import pandas as pd
import numpy as np

data = pd.read_csv("Youtube02-KatyPerry.csv")
data.head()

data.tail()

#Checking for null values
data.isnull().sum()

data.info()

#we are using CONTENT as a independent feature
#the CLASS as dependent label
X = data['CONTENT']
y = data['CLASS']

#Displaying first five records of the feature "CONTENT"
X.head()

#Displaying first five records of the label "CLASS"
y.head()

"""**Splitting our dataset into train and test dataset**"""

#split our dataset into train and test and setting the random state at "123"
#Random state ensures that the splits that you generate are reproducible. Scikit-learn uses random permutations to generate the splits.
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state = 123)
print(X_train.shape)
print(X_test.shape)
print(y_train.shape)
print(y_test.shape)

"""**Vectorizing the dataset**

We use CountVectorizer to convert each content into a matrix of token counts. Moreover, stopwords are removed.
"""

from sklearn.feature_extraction.text import CountVectorizer
vect = CountVectorizer(binary = True, stop_words='english')

## Fitting the CountVectorizer using the training data
vect.fit(X_train)

# Transforming the data frames
X_test_dtm = vect.transform(X_test)
X_train_dtm = vect.fit_transform(X_train)
print(type(X_train_dtm))
print(type(X_test_dtm))

print("Test vector shape : ", X_test_dtm.shape)
print("Train vector shape : ", X_train_dtm.shape)

"""**Logistic Regression Classification model for the Kattyperry data set**"""

#Importing Logistic regression from Sklearn
from sklearn.linear_model import LogisticRegression
model = LogisticRegression(random_state = 123)
#Fitting the model
model.fit(X_train_dtm,y_train)

"""**PREDICTION**"""

#predicting Label for test data
pred = model.predict(X_test_dtm)
#Checking F1 Score
sklearn.metrics.f1_score(y_test, pred, average='binary')

# Importing necessary packages for metric from Sklearn
from sklearn.metrics import accuracy_score,confusion_matrix,classification_report

Logistic_Accuracy = accuracy_score(y_test,pred)
print('Logistic Regression Accuracy Score: ', Logistic_Accuracy)

print('Classification report:\n ' ,classification_report(y_test,pred))

"""**Support Vector Machine(SVM) Classification model (SVC) for the Kattyperry data set**"""

from sklearn import svm
model_s = svm.SVC(random_state = 123)
model_s.fit(X_train_dtm,y_train)

pred_s = model_s.predict(X_test_dtm)
print(pred_s)

SVM_accuracy = accuracy_score(y_test,pred_s)
print('SVM Accuracy Score:', SVM_accuracy)

print(confusion_matrix(y_test, pred_s))
print(classification_report(y_test, pred_s))

"""**Random Forest Classification model for the Katty Perry dataset**"""

#Assigning rf as random forest classifier
rf = sklearn.ensemble.RandomForestClassifier(n_estimators=500, random_state = 125)
#Fiiting the model
rf.fit(X_train_dtm, y_train)

pred_r=rf.predict(X_test_dtm)

RFA = accuracy_score(y_test,pred_r)
print('Random Forest Accuracy Score:', RFA)

sklearn.metrics.f1_score(y_test, pred_r, average='binary')

print(confusion_matrix(y_test,pred_r))
print(classification_report(y_test,pred_r))

"""**Naive Bayes Classification model for the Katty Perry data set**"""

from sklearn.naive_bayes import MultinomialNB
nb = MultinomialNB()

nb.fit(X_train_dtm, y_train) #fit training data

y_pred_nb = nb.predict(X_test_dtm) #Run the Prediction

"""Score the prediction"""

from sklearn.metrics import accuracy_score, confusion_matrix, roc_auc_score
NBA = accuracy_score(y_test, y_pred_nb)
print('Naive Bayes Accuracy Score:', NBA)

confusion_matrix(y_test, y_pred_nb)

X_test[y_pred_nb > y_test] # False positive comment (That is, not_spam marked as spam)

X_test[y_pred_nb < y_test] # False negative comment (Spam comment not detected)

roc_auc_score(y_test, nb.predict_proba(X_test_dtm)[:, 1])

"""**Analysis**"""

X_train_tokens = vect.get_feature_names()

nb.feature_count_ # First line represent not_spam, second line represent span

not_spam_token_count = nb.feature_count_[0, :]
not_spam_token_count

spam_token_count = nb.feature_count_[1, :]
spam_token_count

tokens = pd.DataFrame({'token': X_train_tokens, 'not_spam': not_spam_token_count, 'spam': 
    spam_token_count}).set_index('token')

# Here are 10 random values drawn from the data frame
# Note that these are absolute occurrences
tokens.sample(10, random_state=50)

class_count = nb.class_count_

class_count

tokens.not_spam /= class_count[0]
tokens.spam /= class_count[1]

tokens.sample(10, random_state=50)

words_with_spam_score = ((tokens.spam + 1e-6)/(tokens.not_spam + 1e-6)).sort_values(ascending=False)

# 50 spam words
words_with_spam_score[:50].keys()

# 50 words which are not spam
words_with_spam_score[-50:].keys()

print('Logistic Regression Accuracy Score: ', Logistic_Accuracy)
print('SVM Accuracy Score:', SVM_accuracy)
print('Random Forest Accuracy Score:', RFA)
print('Naive Bayes Accuracy Score:', NBA)

"""**Summary**

In this notebook, we built a machine learning model based on comments for a YouTube video. The training data set had a total of 350 observations. We used a Logistic Regression, SVM, Random Forest and Naive Bayes Classifier in our algorithm, and trained it on 80% of the data set. The model resulted in an accuracy score of **88.57%** for Naive Bayes Classifier, **91.4%** for Random Forest,       **94.28%** for SVM and **92.85%** for Logistic Regression on the test data. We used the model to derive the top 50 most spam and least spam keywords.
"""